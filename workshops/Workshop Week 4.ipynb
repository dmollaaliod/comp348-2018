{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration: Sentiment analysis\n",
    "\n",
    "In this part of the workshop we will walk through a system that uses the Naive Bayes classifiers of NLTK and scikit-learn to predict the review scores of NLTK's corpus of movie reviews. This corpus is used in [NLTK's chapter 6](http://www.nltk.org/book/ch06.html#document-classification). The corpus contains a selection of movie reviews, and a label that indicates whether the review is positive or negative. Classification of reviews as \"positive\" or \"negative\" is a task that is related to [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the movie review corpus\n",
    "\n",
    "The following code reads the movie reviews corpus and shows some statistics on the labels given to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"movie_reviews\")\n",
    "from nltk.corpus import movie_reviews\n",
    "movie_reviews.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative reviews: 1000\n",
      "Number of positive reviews: 1000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of negative reviews:\", len(movie_reviews.fileids('neg')))\n",
    "print(\"Number of positive reviews:\", len(movie_reviews.fileids('pos')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code partitions the movie review corpus into training, devtest, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "random.seed(1234)\n",
    "random.shuffle(documents)\n",
    "threshold1 = int(len(documents)*.6)\n",
    "threshold2 = int(len(documents)*.8)\n",
    "train = documents[:threshold1]\n",
    "devtest = documents[threshold1:threshold2]\n",
    "test = documents[threshold2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document features\n",
    "\n",
    "The following code defines a feature set of the 2000 most frequent non-stop words of the training set. The value of each feature is 1 if the word is present in the document, and 0 otherwise. This is a variant of what is generally called [one-hot encoding](https://en.wikipedia.org/wiki/One-hot). To build the list we will ignore word casing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "c = collections.Counter([w.lower() for (words,category) in train \n",
    "                                   for w in words if w.lower() not in stop])\n",
    "top2000words = [w for (w,count) in c.most_common(2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def document_features(words):\n",
    "    \"Return the document features for an NLTK classifier\"\n",
    "    words_lower = [w.lower() for w in words]\n",
    "    result = dict()\n",
    "    for w in top2000words:\n",
    "        result['has(%s)' % w] = (w in words_lower)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Naive Bayes\n",
    "\n",
    "The following code trains an NLTK Naive Bayes classifier using the training set, and reports the evaluation results on the training set and the dev-test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_features = [(document_features(x), y) for x, y in train]\n",
    "devtest_features = [(document_features(x), y) for x, y in devtest]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7775"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, devtest_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8816666666666667"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the difference in accuracy between the dev-test set and the train set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix features for sklearn\n",
    "\n",
    "The following code defines a second feature extractor that uses the same representation of features as a list of (0,1) which is suitable for sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vector_features(words):\n",
    "    \"Return a vector of features for sklearn\"\n",
    "    words_lower = [w.lower() for w in words]\n",
    "    result = []\n",
    "    for w in top2000words:\n",
    "        if w in words_lower:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn Naive Bayes\n",
    "\n",
    "This code generates the features for sklearn and trains and evaluates a multinomial Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_vectors = [vector_features(x) for x, y in train]\n",
    "devtest_vectors = [vector_features(x) for x, y in devtest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "sklearn_classifier = MultinomialNB()\n",
    "sklearn_classifier.fit(train_vectors, [y for x, y in train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83999999999999997"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = sklearn_classifier.predict(devtest_vectors)\n",
    "true = [y for x, y in devtest]\n",
    "accuracy_score(true, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92000000000000004"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = sklearn_classifier.predict(train_vectors)\n",
    "true = [y for x, y in train]\n",
    "accuracy_score(true, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Both Naive Bayes implementations clearly overfit. What do you think you could do to reduce overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidf with Naive Bayes\n",
    "\n",
    "The following code computes tf.idf of the training set and uses sklearn's multinomial Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that sklearn's `TfidfVectorizer` takes a list of strings as the input, but in our previous experiments we had used the tokenised information, that is, the list of tokens provided by sklearn. We can use `TfidfTransformer` to process a list of tokens (see the [sklearn documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)), \n",
    "but since we haven't covered it in the lectures, let's use the raw strings as our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = [(movie_reviews.raw(fileid), category)\n",
    "         for category in movie_reviews.categories()\n",
    "         for fileid in movie_reviews.fileids(category)]\n",
    "random.seed(1234)\n",
    "random.shuffle(texts)\n",
    "threshold1 = int(len(texts)*.6)\n",
    "threshold2 = int(len(texts)*.8)\n",
    "text_train = texts[:threshold1]\n",
    "text_devtest = texts[threshold1:threshold2]\n",
    "text_test = texts[threshold2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(input='contents', stop_words='english', max_features=2000)\n",
    "text_tfidf_train = tfidf.fit_transform([x for x, y in text_train])\n",
    "text_tfidf_devtest = tfidf.transform([x for x, y in text_devtest])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we used `tfidf.fit_transform` when using the training set, and `tfidf.transform` when using the test set. This is because we use the train set to learn the tfidf parameters \n",
    "(the 2000 most frequent words and their IDF), and then we apply that information when we want to compute the tfidf of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_tfidfclassifier = MultinomialNB()\n",
    "sklearn_tfidfclassifier.fit(text_tfidf_train, [y for x, y in text_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80500000000000005"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score([y for x, y in text_devtest], sklearn_tfidfclassifier.predict(text_tfidf_devtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91416666666666668"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score([y for x, y in text_train], sklearn_tfidfclassifier.predict(text_tfidf_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "The results with tf.idf vectors are similar to the results with one-not encoding. In other applications, tf.idf may improve the results. It didn't happen this time. Why do you think there wasn't much improvement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn\n",
    "\n",
    "## Naive Bayes on Question Segmentation\n",
    "\n",
    "NLTK has a corpus of questions and their question types according to a particular classification scheme (e.g. `DESC` refers to a question expecting a descriptive answer, such as a question starting with \"How\"; `HUM` refers to a question expecting an answer referring to a person). Here's some example of use of the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package qc to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/qc.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"qc\")\n",
    "from nltk.corpus import qc\n",
    "train = qc.tuples(\"train.txt\")\n",
    "test = qc.tuples(\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DESC:manner', 'How did serfdom develop in and then leave Russia ?'),\n",
       " ('ENTY:cremat', 'What films featured the character Popeye Doyle ?'),\n",
       " ('DESC:manner', \"How can I find a list of celebrities ' real names ?\")]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NUM:dist', 'How far is it from Denver to Aspen ?'),\n",
       " ('LOC:city', 'What county is Modesto , California in ?'),\n",
       " ('HUM:desc', 'Who was Galileo ?')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: All question types\n",
    "\n",
    "Write Python code that lists all the possible question types of the training set (remember: _never look at the test set_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ABBR:abb',\n",
       " 'ABBR:exp',\n",
       " 'DESC:def',\n",
       " 'DESC:desc',\n",
       " 'DESC:manner',\n",
       " 'DESC:reason',\n",
       " 'ENTY:animal',\n",
       " 'ENTY:body',\n",
       " 'ENTY:color',\n",
       " 'ENTY:cremat',\n",
       " 'ENTY:currency',\n",
       " 'ENTY:dismed',\n",
       " 'ENTY:event',\n",
       " 'ENTY:food',\n",
       " 'ENTY:instru',\n",
       " 'ENTY:lang',\n",
       " 'ENTY:letter',\n",
       " 'ENTY:other',\n",
       " 'ENTY:plant',\n",
       " 'ENTY:product',\n",
       " 'ENTY:religion',\n",
       " 'ENTY:sport',\n",
       " 'ENTY:substance',\n",
       " 'ENTY:symbol',\n",
       " 'ENTY:techmeth',\n",
       " 'ENTY:termeq',\n",
       " 'ENTY:veh',\n",
       " 'ENTY:word',\n",
       " 'HUM:desc',\n",
       " 'HUM:gr',\n",
       " 'HUM:ind',\n",
       " 'HUM:title',\n",
       " 'LOC:city',\n",
       " 'LOC:country',\n",
       " 'LOC:mount',\n",
       " 'LOC:other',\n",
       " 'LOC:state',\n",
       " 'NUM:code',\n",
       " 'NUM:count',\n",
       " 'NUM:date',\n",
       " 'NUM:dist',\n",
       " 'NUM:money',\n",
       " 'NUM:ord',\n",
       " 'NUM:other',\n",
       " 'NUM:perc',\n",
       " 'NUM:period',\n",
       " 'NUM:speed',\n",
       " 'NUM:temp',\n",
       " 'NUM:volsize',\n",
       " 'NUM:weight']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: All general types\n",
    "\n",
    "The question types have two parts. The first part describes a general type, and the second part defines a subtype. For example, the question type `DESC:manner` belongs to the general `DESC` type and within that type to the `manner` subtype. Let's focus on the general types only. Write Python code that lists all the possible general types (there are 6 of them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ABBR', 'DESC', 'ENTY', 'HUM', 'LOC', 'NUM']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Partition the data\n",
    "There is a train and test data, but for this exercise we want to have a partition into train, dev-test, and test. In this exercise, combine all data into one array and do a 3-way partition into train, dev-test, and test. Make sure that you shuffle the data prior to making the partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Feature extractor\n",
    "\n",
    "Write a feature extractor function that uses individual words as features (\"one-hot encoding\"). To obtain the list of words, use the 100 most frequent words in the training set (since you aren’t supposed to use the dev-test or test sets to extract features). Note that we do not use a list of stop words now since the questions are very short, and some words such as 'how' are useful for question classification but are listed as stop words. **The feature extractor should be suitable for a NLTK classifier.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: NLTK Naive Bayes classifier\n",
    "\n",
    "Train an NLTK Naïve Bayes classifier with the features of the training set, and test it on the testing set. What accuracy do you obtain? Is the system overfittig to the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: sklearn Naive Bayes classifier\n",
    "\n",
    "Convert the feature set to a document matrix suitable for sklearn, and train again using sklearn's Multinomial Naive Bayes classifier. Are the results different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Majority baseline\n",
    "\n",
    "What is the accuracy if we use a majority baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Support Vector Machine\n",
    "Use now sklearn's Support Vector Machines and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Classification errors\n",
    "\n",
    "**This exercise may be useful for some of the advanced tasks of assignment 2.**\n",
    "\n",
    "The following code produces the confusion matrix of the NLTK classifier (read section 3.4 of [Chapter 6 of the NLTK book](http://www.nltk.org/book/ch06.html)). The confusion matrix indicates all errors of classification between different labels. With this information, identify the most common misclassification types. In groups, discuss how you could address the misclassification errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |             E      D                    A |\n",
      "     |      H      N      E      N      L      B |\n",
      "     |      U      T      S      U      O      B |\n",
      "     |      M      Y      C      M      C      R |\n",
      "-----+-------------------------------------------+\n",
      " HUM | <15.5%>  5.3%   1.3%   0.3%   0.6%   0.2% |\n",
      "ENTY |   3.7% <14.0%>  2.8%   0.2%   1.2%      . |\n",
      "DESC |   0.3%   4.1% <15.1%>  1.3%   0.6%      . |\n",
      " NUM |   0.5%   3.9%   1.0% <10.2%>  0.4%      . |\n",
      " LOC |   0.4%   2.9%   1.1%      . <10.8%>     . |\n",
      "ABBR |      .   0.5%   0.8%      .      .  <0.9%>|\n",
      "-----+-------------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qc_devtest_features = [(qc_extractor(q), c) for c, q in q_devtest]\n",
    "devtest_predictions = [qc_classifier.classify(f) for f, l in qc_devtest_features]\n",
    "devtest_labels = [c for c, q in q_devtest]\n",
    "nltk_cm = nltk.ConfusionMatrix(devtest_labels, devtest_predictions)\n",
    "print(nltk_cm.pretty_format(sort_by_count=True, show_percents=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
