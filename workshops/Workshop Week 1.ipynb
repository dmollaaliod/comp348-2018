{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Simple Statistics and NLTK\n",
    "\n",
    "The following exercises use a portion of the Gutenberg corpus that is stored in the corpus dataset of NLTK. [The Project Gutenberg](http://www.gutenberg.org/) is a large collection of electronic books that are out of copyright. These books are free to download for reading, or for our case, for doing a little of corpus analysis.\n",
    "\n",
    "To obtain the list of files of NLTK's Gutenberg corpus, type the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain all words in the entire Gutenberg corpus of NLTK, type the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gutenbergwords = nltk.corpus.gutenberg.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can find the total number of words, and the first 10 words (do not attempt to display all the words or your computer will freeze!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2621613"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gutenbergwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenbergwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also find the words of just a selection of documents, as shown below. For more details of what information you can extract from this corpus, read the \"Gutenberg corpus\" section of the [NLTK book chapter 2](http://www.nltk.org/book_1ed/ch02.html), section 2.1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1\n",
    "*Write Python code that prints the 10 most common words in each of the documents of the Gutenberg corpus. Can you identify any similarities among them?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen-emma.txt\n",
      "[(',', 11454), ('.', 6928), ('to', 5183), ('the', 4844), ('and', 4672), ('of', 4279), ('I', 3178), ('a', 3004), ('was', 2385), ('her', 2381)]\n",
      "austen-persuasion.txt\n",
      "[(',', 6750), ('the', 3120), ('to', 2775), ('.', 2741), ('and', 2739), ('of', 2564), ('a', 1529), ('in', 1346), ('was', 1330), (';', 1290)]\n",
      "austen-sense.txt\n",
      "[(',', 9397), ('to', 4063), ('.', 3975), ('the', 3861), ('of', 3565), ('and', 3350), ('her', 2436), ('a', 2043), ('I', 2004), ('in', 1904)]\n",
      "bible-kjv.txt\n",
      "[(',', 70509), ('the', 62103), (':', 43766), ('and', 38847), ('of', 34480), ('.', 26160), ('to', 13396), ('And', 12846), ('that', 12576), ('in', 12331)]\n",
      "blake-poems.txt\n",
      "[(',', 680), ('the', 351), ('.', 201), ('And', 176), ('and', 169), ('of', 131), ('I', 130), ('in', 116), ('a', 108), (\"'\", 104)]\n",
      "bryant-stories.txt\n",
      "[(',', 3481), ('the', 3086), ('and', 1873), ('.', 1817), ('to', 1165), ('a', 988), ('\"', 900), ('he', 872), ('of', 801), ('was', 706)]\n",
      "burgess-busterbrown.txt\n",
      "[('.', 823), (',', 822), ('the', 639), ('he', 562), ('and', 484), ('to', 426), (\"'\", 401), ('of', 326), ('that', 285), ('a', 275)]\n",
      "carroll-alice.txt\n",
      "[(',', 1993), (\"'\", 1731), ('the', 1527), ('and', 802), ('.', 764), ('to', 725), ('a', 615), ('I', 543), ('it', 527), ('she', 509)]\n",
      "chesterton-ball.txt\n",
      "[(',', 4547), ('the', 4523), ('.', 3589), ('of', 2529), ('and', 2488), ('a', 2184), ('\"', 1751), ('to', 1558), ('in', 1355), ('that', 1120)]\n",
      "chesterton-brown.txt\n",
      "[('the', 4321), (',', 4069), ('.', 2784), ('of', 2087), ('and', 2074), ('a', 2074), ('\"', 1461), ('to', 1378), ('in', 1205), ('was', 1141)]\n",
      "chesterton-thursday.txt\n",
      "[(',', 3488), ('the', 3291), ('.', 2717), ('a', 1713), ('of', 1710), ('and', 1568), ('\"', 1336), ('to', 1045), ('in', 888), ('I', 885)]\n",
      "edgeworth-parents.txt\n",
      "[(',', 15219), ('the', 7149), ('.', 6945), ('to', 5150), ('and', 4769), ('\"', 3880), ('of', 3730), ('I', 3656), (\"'\", 3293), ('a', 3017)]\n",
      "melville-moby_dick.txt\n",
      "[(',', 18713), ('the', 13721), ('.', 6862), ('of', 6536), ('and', 6024), ('a', 4569), ('to', 4542), (';', 4072), ('in', 3916), ('that', 2982)]\n",
      "milton-paradise.txt\n",
      "[(',', 10198), ('and', 2799), ('the', 2505), (';', 2317), ('to', 1758), ('of', 1486), ('.', 1254), ('in', 1083), ('his', 986), ('with', 876)]\n",
      "shakespeare-caesar.txt\n",
      "[(',', 2204), ('.', 1296), ('I', 531), ('the', 502), (':', 499), ('and', 409), (\"'\", 384), ('to', 370), ('you', 342), ('of', 336)]\n",
      "shakespeare-hamlet.txt\n",
      "[(',', 2892), ('.', 1886), ('the', 860), (\"'\", 729), ('and', 606), ('of', 576), ('to', 576), (':', 565), ('I', 553), ('you', 479)]\n",
      "shakespeare-macbeth.txt\n",
      "[(',', 1962), ('.', 1235), (\"'\", 637), ('the', 531), (':', 477), ('and', 376), ('I', 333), ('of', 315), ('to', 311), ('?', 241)]\n",
      "whitman-leaves.txt\n",
      "[(',', 17713), ('the', 8814), ('and', 4797), ('of', 4127), ('I', 2932), (\"'\", 2362), ('to', 1930), ('-', 1774), ('.', 1769), ('in', 1714)]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2\n",
    "*Find the words with length of at least 7 characters in the complete Gutenberg corpus.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Woodhouse',\n",
       " 'handsome',\n",
       " 'comfortable',\n",
       " 'disposition',\n",
       " 'blessings',\n",
       " 'existence',\n",
       " 'distress',\n",
       " 'youngest',\n",
       " 'daughters',\n",
       " 'affectionate']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = # ... write your code here ...\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3\n",
    "*Find the words that are longer than 7 characters and occur more than 7 times in the complete Gutenberg corpus.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Woodhouse', 313),\n",
       " ('handsome', 130),\n",
       " ('comfortable', 108),\n",
       " ('disposition', 73),\n",
       " ('blessings', 24),\n",
       " ('existence', 47),\n",
       " ('distress', 111),\n",
       " ('youngest', 42),\n",
       " ('daughters', 319),\n",
       " ('affectionate', 56)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ... write your code here and write the result in frequentwords ...\n",
    "frequentwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.4\n",
    "*Find the average number of words across the documents of the NLTK Gutenberg corpus.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145645.16666666666"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.5\n",
    "*Find the Gutenberg document that has the longest average word length.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document with largest average word length is milton-paradise.txt with word length 4.835734572682675\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.6\n",
    "*Find the 10 most frequent bigrams in the entire Gutenberg corpus.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((',', 'and'), 41294),\n",
       " (('of', 'the'), 18912),\n",
       " (('in', 'the'), 9793),\n",
       " ((\"'\", 's'), 9781),\n",
       " ((';', 'and'), 7559),\n",
       " (('and', 'the'), 6432),\n",
       " (('the', 'LORD'), 5964),\n",
       " ((',', 'the'), 5957),\n",
       " ((',', 'I'), 5677),\n",
       " ((',', 'that'), 5352)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.7\n",
    "*Find the most frequent bigram that begins with \"Moby\" in Herman Melville's \"Moby Dick\".*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Moby', 'Dick'), 83), (('Moby', '-'), 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text Preprocessing with NLTK\n",
    "The following exercises will ask several questions about tokens, stems, and parts of speech.\n",
    "\n",
    "### Exercise 2.1\n",
    "*What is the sentence with the largest number of tokens in Austen's \"Emma\"?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "The longest sentence is\n",
      "While he lived, it must be only an engagement;\n",
      "but she flattered herself, that if divested of the danger of\n",
      "drawing her away, it might become an increase of comfort to him.--\n",
      "How to do her best by Harriet, was of more difficult decision;--\n",
      "how to spare her from any unnecessary pain; how to make\n",
      "her any possible atonement; how to appear least her enemy?--\n",
      "On these subjects, her perplexity and distress were very great--\n",
      "and her mind had to pass again and again through every bitter\n",
      "reproach and sorrowful regret that had ever surrounded it.--\n",
      "She could only resolve at last, that she would still avoid a\n",
      "meeting with her, and communicate all that need be told by letter;\n",
      "that it would be inexpressibly desirable to have her removed just\n",
      "now for a time from Highbury, and--indulging in one scheme more--\n",
      "nearly resolve, that it might be practicable to get an invitation\n",
      "for her to Brunswick Square.--Isabella had been pleased with Harriet;\n",
      "and a few weeks spent in London must give her some amusement.--\n",
      "She did not think it in Harriet's nature to escape being benefited\n",
      "by novelty and variety, by the streets, the shops, and the children.--\n",
      "At any rate, it would be a proof of attention and kindness in herself,\n",
      "from whom every thing was due; a separation for the present; an averting\n",
      "of the evil day, when they must all be together again.\n",
      "The mumber of words of the longest sentence is 275\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2\n",
    "*What is the most frequent part of speech in Austen's \"Emma\"?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 19346), ('IN', 17877), ('PRP', 15600), ('RB', 12985), ('DT', 12738)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3\n",
    "*What is the number of distinct stems in Austen's \"Emma\"?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Austen's Emma has 5450 distinct stems\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4\n",
    "*What is the most ambiguous stem in Austen's \"Emma\"? (meaning, which stem in Austen's \"Emma\" maps to the largest number of distinct tokens?)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most ambiguous stem is respect with words {'respectfully', 'respectable', 'respecting', 'respectability', 'Respect', 'respectful', 'respectably', 'respected', 'respect', 'respects', 'respective'}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Regular Expressions\n",
    "The aim of this section is to develop a regular expression that detects all the numerical expressions in the Brown corpus. The Brown corpus is described in the [NLTK book chapter 2](http://www.nltk.org/book_1ed/ch02.html), section 2.1.\n",
    "\n",
    "The Brown corpus is a corpus annotated with the parts of speech. In the following exercises, you will concentrate on the words tagged with labels that begin with the `CD` tag. This tag stands for \"cardinal numeral\" and indicates that the token is a number. For the full list of tags in the Brown corpus you can see the [Wikipedia entry](https://en.wikipedia.org/wiki/Brown_Corpus#Part-of-speech_tags_used).\n",
    "\n",
    "The following Python code shows the first 5 words of the \"news\" category of the Brown corpus. You'll see that it is a list of pairs, where each pair is a word and a tag. The tag has two components separated with `-`. We will focus on the labels that begin with `'CD'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('brown')\n",
    "tagged = nltk.corpus.brown.tagged_words(categories='news')\n",
    "tagged[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code stores the list of unannotated tokens in the variable `tokens`. Note that we could have used `nltk.corpus.brown.words` but build this list but the code below makes sure that the list mirrors the list `tagged'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [w for (w,t) in tagged]\n",
    "tokens[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1\n",
    "*Write code that finds all the numbers (items tagged with `'CD'`) from the list \"`tagged`\" and stores them in a new variable \"`numbers`\". How many numbers are there? How many different numbers are there?*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2020 numbers\n",
      "There are 460 distinct numbers\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2\n",
    "*Complete the following function that finds all the items in `tokens` that match the following regular expression: `^[0-9]+$`. The function should return the result as a list of pairs `(word,annotation)` so that, if the word is a number, the annotation is `'CD'`, and if it is not a number, the annotation is `''`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def annotateNum(listtokens):\n",
    "    \"\"\"Annotate the list of tokens to identify the numbers.\n",
    "    Example of run:\n",
    "    >>> annotateNum(['the','number','5'])\n",
    "    [('the', ''), ('number', ''), ('5', 'CD')]\n",
    "    \"\"\"\n",
    "    # ... write your code here ...\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', ''), ('number', ''), ('5', 'CD')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotateNum(['the','number','5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code computes the recall and precision of the annotations.\n",
    "\n",
    "1. The *recall* is the ratio of numbers that are tagged correctly.\n",
    "2. The *precision* is the ratio of tagged tokens that are numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(result,tagged):\n",
    "    assert len(result) == len(tagged) # This is a check that the length of the result and tagged are equal\n",
    "    correct = [result[i][0] for i in range(len(result)) if result[i][1][:2] == 'CD' and tagged[i][1][:2] == 'CD']\n",
    "    numbers_result = [result[i][0] for i in range(len(result)) if result[i][1][:2] == 'CD']\n",
    "    numbers_tagged = [tagged[i][0] for i in range(len(tagged)) if tagged[i][1][:2] == 'CD']\n",
    "    print(\"Recall:\",len(correct)/len(numbers_tagged))\n",
    "    print(\"Precision:\",len(correct)/len(numbers_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.5304428044280443\n",
      "Precision: 0.9982638888888888\n"
     ]
    }
   ],
   "source": [
    "evaluate(annotateNum(tokens),tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code returns the mistakes that you produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def false_positives(result,tagged):\n",
    "    \"Return the non-numbers that were tagged as numbers\"\n",
    "    return [tagged[i] for i in range(len(result)) if result[i][1]== 'CD' and tagged[i][1][:2] != 'CD']\n",
    "def false_negatives(result,tagged):\n",
    "    \"Return the numbers that were not tagged as numbers\"\n",
    "    return [result[i] for i in range(len(result)) if result[i][1]!= 'CD' and tagged[i][1][:2] == 'CD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3', 'OD'), ('3', 'OD-TL')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_positives(annotateNum(tokens),tagged)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('two', ''),\n",
       " ('one', ''),\n",
       " ('two', ''),\n",
       " ('Four', ''),\n",
       " ('one', ''),\n",
       " ('one', ''),\n",
       " ('one', ''),\n",
       " ('two', ''),\n",
       " ('Five', ''),\n",
       " ('three', '')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_negatives(annotateNum(tokens),tagged)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3\n",
    "**This exercise is optional for this workshop but it will be very useful for the sort of work that you would normally do as a developer of text applications. This kind of workflow can also be useful for the assignments.**\n",
    "\n",
    "*Concentrate on your worst figure, either recall and precision, and try to improve it:*\n",
    "\n",
    "1. *If recall is low, list all the numbers that your system failed to detect (the false negatives). With those numbers in mind, update your regular expression.*\n",
    "2. *If precision is low, list all the tokens that your system erroneously classified as numbers (the false positives). The update your regular expression to cover those words.*\n",
    "\n",
    "*Test your new regular expression on the full Brown corpus, compute recall and precision, and see what you can improve. Do this several times and try to get better results.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
