{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Python and NLTK for Text Processing\n",
    "*Submission deadline: Friday 16 March 2018, 11:00pm*\n",
    "\n",
    "## Objectives of this assignment\n",
    "\n",
    "In this assignment you will practice with the use of Python packages for text processing as a first step towards implementing real-world document processing systems.\n",
    "\n",
    "The deadline of this assignment is before census date, so it can serve as a diagnostic test so that you can determine if you want to remain in the unit or withdraw without penalty.\n",
    "\n",
    "Below are the questions of this assignment. They are in the format of a Jupyter notebook so that you can use this notebook to work on your solution. Write the final solution in a standalone Python file as described in the \"submission\" section by the end of these assignment specifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/gideonsacks/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/gideonsacks/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', ...]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word counts (1 mark)\n",
    "Implement a function that returns a vector of word counts in a given text, given a vector of words. For your solution you may use third-party modules if you wish. As part of this exercise you will need to split the text into words. When you do it, please note that NLTK's tokeniser works best when it takes sentences as their input. Thus, to tokenise a text that has multiple sentences it is best first to split the text into sentences, and then tokenise each sentence. Look at the lecture notes and exercises of the week 1 workshop for examples of how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 0, 1, 0]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n",
    "import collections\n",
    "from collections import Counter\n",
    "emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "def word_counts(text, words):\n",
    "    \n",
    "    if(isinstance(text, str)):\n",
    "        text = word_tokenize(text)\n",
    "        #for x in string.punctuation:\n",
    "            #text= text.replace(x,\"\")\n",
    "        #text = text.split()\n",
    "    #text = word_tokenize(text)\n",
    "    data = Counter(text)\n",
    "    i=0\n",
    "    datacount = dict()\n",
    "\n",
    "    for word in words:\n",
    "        datacount[i] = data[word]\n",
    "        i=i+1\n",
    "    return list(datacount.values())\n",
    "  \n",
    "\n",
    "\n",
    "word_counts(emma, ['the', 'a'])\n",
    "word_counts(\"Here is sentence one. cant can't Here is sentence two.\", ['Here', 'two', 'three', 'cant',\"can't\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  PoS counts (1 mark)\n",
    "Implement a function that returns a vector of counts of parts of speech, given a vector of parts of speech. To determine the parts of speech, use NLTK's `pos_tag_sents` function using the `'universal'` tag set. See the lecture notes and practical exercises from week 1 for details of how to use `pos_tag_sents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14524, 35747]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "def pos_counts(text, pos_list):\n",
    "    \n",
    "    sorted_text = sorted(text)\n",
    "    sorted_text = [''.join(c for c in s if c not in string.punctuation) for s in sorted_text]\n",
    "    sorted_text = [s for s in sorted_text if s]\n",
    "    text = nltk.pos_tag(sorted_text, tagset=\"universal\")\n",
    "    \n",
    "    datacount = dict()\n",
    "    i=0\n",
    "    j=0\n",
    "    sum=0\n",
    "    for word in pos_list:\n",
    "        i=0\n",
    "        for lists in text:\n",
    "            if(text[i][1]==word):\n",
    "                sum=sum+1\n",
    "            i=i+1\n",
    "        datacount[j] = sum\n",
    "        j=j+1\n",
    "        sum=0\n",
    "    return list(datacount.values())\n",
    "pos_counts(emma, ['DET', 'NOUN'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000',\n",
       " '000',\n",
       " '10',\n",
       " '10',\n",
       " '1816',\n",
       " '23rd',\n",
       " '24th',\n",
       " '26th',\n",
       " '28th',\n",
       " '28th',\n",
       " '7th',\n",
       " '8th',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'Abbey',\n",
       " 'Abbey',\n",
       " 'Abbey',\n",
       " 'Abbey',\n",
       " 'Abbey',\n",
       " 'Abbey',\n",
       " 'Abbey',\n",
       " 'Abbey',\n",
       " 'Abbey',\n",
       " 'Abbey',\n",
       " 'Abbey',\n",
       " 'Abbey',\n",
       " 'Abbey',\n",
       " 'Abbey',\n",
       " 'Abbey',\n",
       " 'Abbey',\n",
       " 'Abbey',\n",
       " 'Abbey',\n",
       " 'Abbey',\n",
       " 'Abbey',\n",
       " 'Abbey',\n",
       " 'Abbey',\n",
       " 'Abbey',\n",
       " 'Abbey',\n",
       " 'Abbey',\n",
       " 'Abbey',\n",
       " 'Abbey',\n",
       " 'Abbey',\n",
       " 'Abbey',\n",
       " 'Abbey',\n",
       " 'Abbey',\n",
       " 'Abbots',\n",
       " 'Abdy',\n",
       " 'Abominable',\n",
       " 'About',\n",
       " 'About',\n",
       " 'About',\n",
       " 'Absence',\n",
       " 'Absolute',\n",
       " 'Absolutely',\n",
       " 'Absurd',\n",
       " 'According',\n",
       " 'Accordingly',\n",
       " 'Acquit',\n",
       " 'Actually',\n",
       " 'Actually',\n",
       " 'Adelaide',\n",
       " 'Adelaide',\n",
       " 'Adopt',\n",
       " 'After',\n",
       " 'After',\n",
       " 'After',\n",
       " 'After',\n",
       " 'After',\n",
       " 'After',\n",
       " 'After',\n",
       " 'After',\n",
       " 'After',\n",
       " 'After',\n",
       " 'After',\n",
       " 'After',\n",
       " 'After',\n",
       " 'After',\n",
       " 'After',\n",
       " 'After',\n",
       " 'After',\n",
       " 'After',\n",
       " 'After',\n",
       " 'After',\n",
       " 'After',\n",
       " 'Agreed',\n",
       " 'Agricultural',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Ah',\n",
       " 'Aladdin',\n",
       " 'Alas',\n",
       " 'Alas',\n",
       " 'Alas',\n",
       " 'Alderneys',\n",
       " 'All',\n",
       " 'All',\n",
       " 'All',\n",
       " 'All',\n",
       " 'All',\n",
       " 'All',\n",
       " 'All',\n",
       " 'All',\n",
       " 'All',\n",
       " 'All',\n",
       " 'Almane',\n",
       " 'Almost',\n",
       " 'Although',\n",
       " 'Altogether',\n",
       " 'Always',\n",
       " 'Always',\n",
       " 'Always',\n",
       " 'Am',\n",
       " 'Am',\n",
       " 'Am',\n",
       " 'Ambition',\n",
       " 'Amiable',\n",
       " 'An',\n",
       " 'An',\n",
       " 'An',\n",
       " 'An',\n",
       " 'An',\n",
       " 'An',\n",
       " 'An',\n",
       " 'An',\n",
       " 'An',\n",
       " 'An',\n",
       " 'An',\n",
       " 'An',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'And',\n",
       " 'Angry',\n",
       " 'Anna',\n",
       " 'Anne',\n",
       " 'Anne',\n",
       " 'Anne',\n",
       " 'Another',\n",
       " 'Another',\n",
       " 'Another',\n",
       " 'Another',\n",
       " 'Another',\n",
       " 'Another',\n",
       " 'Another',\n",
       " 'Another',\n",
       " 'Anxious',\n",
       " 'Any',\n",
       " 'Any',\n",
       " 'Any',\n",
       " 'Anywhere',\n",
       " 'Apologies',\n",
       " 'Approve',\n",
       " 'April',\n",
       " 'April',\n",
       " 'April',\n",
       " 'Are',\n",
       " 'Are',\n",
       " 'Are',\n",
       " 'Are',\n",
       " 'Are',\n",
       " 'Are',\n",
       " 'Are',\n",
       " 'Are',\n",
       " 'Arthur',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'As',\n",
       " 'Assured',\n",
       " 'Astley',\n",
       " 'Astley',\n",
       " 'Astley',\n",
       " 'Astley',\n",
       " 'Astonished',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'At',\n",
       " 'August',\n",
       " 'August',\n",
       " 'August',\n",
       " 'Augusta',\n",
       " 'Augusta',\n",
       " 'Augusta',\n",
       " 'Augusta',\n",
       " 'Aunt',\n",
       " 'Austen',\n",
       " 'Aye',\n",
       " 'Aye',\n",
       " 'Aye',\n",
       " 'Aye',\n",
       " 'Aye',\n",
       " 'Aye',\n",
       " 'Aye',\n",
       " 'Aye',\n",
       " 'Aye',\n",
       " 'Aye',\n",
       " 'Aye',\n",
       " 'Aye',\n",
       " 'Aye',\n",
       " 'Bad',\n",
       " 'Bad',\n",
       " 'Balls',\n",
       " 'Baly',\n",
       " 'Barnes',\n",
       " 'Baronne',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bates',\n",
       " 'Bateses',\n",
       " 'Bateses',\n",
       " 'Bateses',\n",
       " 'Bateses',\n",
       " 'Bateses',\n",
       " 'Bath',\n",
       " 'Bath',\n",
       " 'Bath',\n",
       " 'Bath',\n",
       " 'Bath',\n",
       " 'Bath',\n",
       " 'Bath',\n",
       " 'Bath',\n",
       " 'Bath',\n",
       " 'Bath',\n",
       " 'Bath',\n",
       " 'Bath',\n",
       " 'Bath',\n",
       " 'Bath',\n",
       " 'Bath',\n",
       " 'Bath',\n",
       " 'Bath',\n",
       " 'Bath',\n",
       " 'Bath',\n",
       " 'Be',\n",
       " 'Be',\n",
       " 'Be',\n",
       " 'Be',\n",
       " 'Be',\n",
       " 'Bear',\n",
       " 'Beautiful',\n",
       " 'Beavers',\n",
       " 'Before',\n",
       " 'Before',\n",
       " 'Before',\n",
       " 'Before',\n",
       " 'Before',\n",
       " 'Before',\n",
       " 'Before',\n",
       " 'Beg',\n",
       " 'Behold',\n",
       " 'Behold',\n",
       " 'Behold',\n",
       " 'Being',\n",
       " 'Being',\n",
       " 'Believe',\n",
       " 'Bella',\n",
       " 'Bella',\n",
       " 'Bella',\n",
       " 'Besides',\n",
       " 'Besides',\n",
       " 'Besides',\n",
       " 'Better',\n",
       " 'Better',\n",
       " 'Better',\n",
       " 'Better',\n",
       " 'Better',\n",
       " 'Better',\n",
       " 'Better',\n",
       " 'Between',\n",
       " 'Between',\n",
       " 'Beyond',\n",
       " 'Beyond',\n",
       " 'Bickerton',\n",
       " 'Bickerton',\n",
       " 'Bickerton',\n",
       " 'Bird',\n",
       " 'Birmingham',\n",
       " 'Birmingham',\n",
       " 'Birth',\n",
       " 'Bless',\n",
       " 'Bless',\n",
       " 'Bless',\n",
       " 'Bless',\n",
       " 'Blessed',\n",
       " 'Boarding',\n",
       " 'Bond',\n",
       " 'Bond',\n",
       " 'Books',\n",
       " 'Both',\n",
       " 'Both',\n",
       " 'Bought',\n",
       " 'Box',\n",
       " 'Box',\n",
       " 'Box',\n",
       " 'Box',\n",
       " 'Box',\n",
       " 'Box',\n",
       " 'Box',\n",
       " 'Box',\n",
       " 'Box',\n",
       " 'Box',\n",
       " 'Box',\n",
       " 'Box',\n",
       " 'Box',\n",
       " 'Box',\n",
       " 'Box',\n",
       " 'Box',\n",
       " 'Box',\n",
       " 'Box',\n",
       " 'Bragge',\n",
       " 'Bragge',\n",
       " 'Bragge',\n",
       " 'Bragge',\n",
       " 'Bragge',\n",
       " 'Bragge',\n",
       " 'Bragge',\n",
       " 'Bragges',\n",
       " 'Bragges',\n",
       " 'Braithwaites',\n",
       " 'Break',\n",
       " 'Bristol',\n",
       " 'Bristol',\n",
       " 'Bristol',\n",
       " 'Bristol',\n",
       " 'Bristol',\n",
       " 'Bristol',\n",
       " 'Bristol',\n",
       " 'Bristol',\n",
       " 'Broadway',\n",
       " 'Broadwood',\n",
       " 'Broadwood',\n",
       " 'Brother',\n",
       " 'Brown',\n",
       " 'Brunswick',\n",
       " 'Brunswick',\n",
       " 'Brunswick',\n",
       " 'Brunswick',\n",
       " 'Brunswick',\n",
       " 'Brunswick',\n",
       " 'Brunswick',\n",
       " 'Brunswick',\n",
       " 'Brunswick',\n",
       " 'Brunswick',\n",
       " 'Brunswick',\n",
       " 'Business',\n",
       " 'Busy',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " 'But',\n",
       " ...]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "sorted_emma = sorted(emma)\n",
    "sorted_emma = [''.join(c for c in s if c not in string.punctuation) for s in sorted_emma]\n",
    "sorted_emma = [s for s in sorted_emma if s]\n",
    "#emma = nltk.pos_tag(sorted_emma, tagset=\"universal\")\n",
    "sorted_emma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readability (1 mark)\n",
    "A popular formula to measure the readability of a document is the [Flesh reading-ease test (FRES)](https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests), which gives higher scores to texts that are easier to read. According to Wikipedia, the formula is: \n",
    "\n",
    "![formula](https://wikimedia.org/api/rest_v1/media/math/render/svg/bd4916e193d2f96fa3b74ee258aaa6fe242e110e)\n",
    "\n",
    "Write a function that returns the FRES of a text. To help you in this exercise, below is a simple function that you can use to approximate the number of syllables in a word. This function is based on the calculation of the word length used for the [Porter stemmer](https://tartarus.org/martin/PorterStemmer/def.txt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "VC = re.compile('[aeiou]+[^aeiou]+', re.I)\n",
    "def count_syllables(word):\n",
    "    return len(VC.findall(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced task (2 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the last practical exercises of week 1 you were asked to identify all the cardinal numbers in a list of tokens. In this advanced task, you will need to identify all the **ordinal numbers** such as \"first\", \"22nd\", etc. We will use the Brown corpus, which, as you know, is annotated with the parts of speech. The Brown corpus tags for ordinal numbers begin with 'OD'. The following code counts all the tokens tagged as ordinal numbers in the \"news\" section of NLTK's Brown corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "tagged = nltk.corpus.brown.tagged_words(categories='news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'OD': 309, 'OD-HL': 1, 'OD-TL': 30})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "c = collections.Counter([t for w, t in tagged if t[:2] == 'OD'])\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that annotates all ordinal numbers with 'OD' and everything else with the empty string ''. As an example to get you started, you can reuse this code which uses a simple regular expression that tags all tokens that end in 'st' ,'nd', 'rd' and 'th'. The function will correctly label words such as 'first' and 'fifth' but it will incorrectly label words like 'tooth' and 'and':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "regexp = re.compile('.*(st|nd|rd|th)$')\n",
    "def annotateOD(listoftokens):\n",
    "    result = []\n",
    "    for t in listoftokens:\n",
    "        if regexp.match(t):\n",
    "            result.append((t, 'OD'))\n",
    "        else:\n",
    "            result.append((t, ''))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', ''), ('second', 'OD'), ('tooth', 'OD')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotateOD(\"the second tooth\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the function, the automarking system will compute the F1 score using code like this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_f1(result, tagged):\n",
    "    assert len(result) == len(tagged) # This is a check that the length of the result and tagged are equal\n",
    "    correct = [result[i][0] for i in range(len(result)) if result[i][1][:2] == 'OD' and tagged[i][1][:2] == 'OD']\n",
    "    numbers_result = [result[i][0] for i in range(len(result)) if result[i][1][:2] == 'OD']\n",
    "    numbers_tagged = [tagged[i][0] for i in range(len(tagged)) if tagged[i][1][:2] == 'OD']\n",
    "    if len(numbers_tagged) > 0:\n",
    "        r = len(correct)/len(numbers_tagged)\n",
    "    else:\n",
    "        r = 0.0\n",
    "    if len(numbers_result) > 0:\n",
    "        p = len(correct)/len(numbers_result)\n",
    "    else:\n",
    "        p = 0.0\n",
    "    return 2*r*p/(r+p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11333103685842233"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [t for t, w in tagged]\n",
    "result = annotateOD(words)\n",
    "compute_f1(result, tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good approach to identify what you could do to increase the F1 score of your system would be to look at the false positives and the false negatives. Feel free to adapt the code from the practical exercises to identify false positives and false negatives.\n",
    "\n",
    "The advanced task will be marked as follows:\n",
    "\n",
    "* F1 > 0.9: 2 marks\n",
    "* F1 > 0.3: 1 mark\n",
    "* F1 < 0.3: 0 marks\n",
    "\n",
    "**Note that your code should not use any large lists of words, and should not use any part of speech taggers.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "Submit a single Python file with the solutions to all the questions. We provide a template defines all the functions as stubs. Make sure that you do not change the names and argument structure of the functions, since the submission will use an automatic marker that relies on these exact names and argument structure. The template we provide includes a few simple tests using [Python's doctest](https://docs.python.org/3/library/doctest.html) environment. These tests are there to help you, but note that we may use a separate set of tests when we assess your submission. It is your responsibility to run your own tests, in addition to the doctests provided.\n",
    "\n",
    "The submission must be a single Python file. Do not submit several files or a zip file since the automarker would not know what to do with your submission.\n",
    "\n",
    "Note that the deadline is a hard deadline and there will be a penalty of one mark per day of late submission. In addition, since the submission date is a week before the census date of 26 of March 2018, late submissions might not be assessed before census date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
